---
title: "DS 2870: Homework 9"
author: "Dan Schiefen"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      #warning = F,
                      #message = F,
                      fig.align = 'center')

pacman::p_load(tidyverse, class, mlbench)

```


## Homework 9 Description

This assignment only has one objective: Write a function that will tune the value of $k$ for kNN classification for any data set given to it.

You'll need to do the following:

1) Name the function `knn_search()` with the following arguments:
    - `X` = the data set of predictors
    - `class` = the class/category for rows of `X`
    - `k_grid` = the different values of $k$ to search over

2) Perform a grid search across k for k-nearest neighbors classification when the data are both standardize and normalize.

3) Returns two objects:
  a) `k_error`: A data frame with the following columns
      - `k`: the values of $k$ supplied by `k_grid`
      - `rescale`: the rescale method ('norm' and 'stan').
      - `error`: The misclassification rate for the choice of $k$ and rescale method
  
  b) `gg_serch`: The resulting graph of the grid search with:
      - `x = k`
      - `y = error`
      - `color = rescale`
      - a dashed line at the value of `k` that minimizes the error rate with color that matches the rescale method

There are two subsequent code chunks you can use check your function. The two chunks below that have their output hidden in Brightspace and will be used to grade your function.


```{r knn_search_function}
# Normalize Function
normalize <- function(x){return((x - min(x)) / (max(x) - min(x)))}

knn_search <- 
  function(X, class, k_grid) {
    #Create normalized dataset
    X_norm <- 
      X |> 
      dplyr::select(where(is.numeric)) |> 
      mutate(
        across(
          .cols = where(is.numeric),
          .fns = normalize
          )
        )
      
    #Create standardized dataset
    X_stan <- 
      X |> 
      dplyr::select(where(is.numeric)) |> 
      mutate(
        across(
          .cols = where(is.numeric),
          .fns = scale
          )
        )
    
    #Initialize data table
    k_error <- 
      data.frame(
        k = k_grid,
        norm = -1,
        stan = -1)
    
    for (i in 1:nrow(k_error)) {
      #perform kNN for the ith choice of K on the normalized data set
      norm_loop <- 
        knn.cv(
          train = X_norm,
          cl = class,
          k = k_grid[i]
          )
      
      #perform kNN for the ith choice of K on the standardized data set
      stan_loop <- 
        knn.cv(
          train = X_stan,
          cl = class,
          k = k_grid[i]
          )
      
      #Save to table
      k_error$norm[i] <- mean(norm_loop != class)
      k_error$stan[i] <- mean(stan_loop != class)
    }
    
    #pivot data to put rescale into same column
    k_error <- 
      k_error |> 
      pivot_longer(
        cols = -k,
        names_to = "rescale",
        values_to = "error"
      )
    
    #Finding the k value(s) which minimize error
    min_k <- 
      k_error |> 
      slice_min(
        order_by = error, 
        n = 1
        )
    
    #creating graph
    gg_search <-
      ggplot(
        data = k_error,
        mapping = aes(x = k,
                      y = error,
                      color = rescale)
        ) +
      geom_line() +
      geom_vline(
        data = min_k,
        mapping = aes(
          xintercept = k,
          color = rescale),
        linetype = 2,
        show.legend = F
      ) +
      theme_bw()
    
    return(list(k_error = k_error, gg_search = gg_search))
}

```




#### Check 1: iris data

```{r check1}
RNGversion('4.1.0');set.seed(2870)

check1_knn <- 
  knn_search(
    X = iris[, -5], 
    class = iris$Species,
    k_grid = 5:50
  )

# Returned data frame
check1_knn$k_error |> 
  slice_sample(n = nrow(check1_knn$k_error)) |> 
  tibble()

# Graph
check1_knn$gg_search + ggtitle('Iris data')

```





#### Check 2: penguins data

```{r check2}
RNGversion('4.1.0');set.seed(2870)

check2_knn <- 
  knn_search(
    X = drop_na(palmerpenguins::penguins[, 5:6]), 
    class = palmerpenguins::penguins |> dplyr::select(1, 5:6)|> drop_na() |> pull(species),
    k_grid = 1:100
  )

# Returned data frame
check2_knn$k_error |> 
  slice_sample(n = nrow(check2_knn$k_error)) |> 
  tibble()

# Graph
check2_knn$gg_search + ggtitle('Penguin data')

```







#### Check 3: 

```{r check3}
RNGversion('4.1.0');set.seed(2870)

mpg2 <- 
  mpg |> 
  filter(class %in% c('compact', 'midsize', 'suv', 'pickup')) |> 
  dplyr::select(class, displ, cty, hwy)

check3_knn <- 
  knn_search(
    X = mpg2[, 2:4],
    class = mpg2$class,
    k_grid = 1:100
  )

# Returned data frame
check3_knn$k_error |> 
  slice_sample(n = nrow(check3_knn$k_error)) |> 
  tibble()

# Graph
check3_knn$gg_search + ggtitle('Car data')

```


#### Check 4: 

```{r check4}
RNGversion('4.1.0');set.seed(2870)

cancer <- 
  read.csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', header = F) |> 
  dplyr::select(-V1) |> 
  rename(diagnosis = V2)

check4_knn <- 
  knn_search(
    X = cancer[, -1],
    class = cancer$diagnosis,
    k_grid = 1:300
  )

# Returned data frame
check4_knn$k_error |> 
  slice_sample(n = nrow(check4_knn$k_error)) |> 
  tibble()

# Graph
check4_knn$gg_search + ggtitle('Cancer data')

```
